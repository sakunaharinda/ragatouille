{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "The basic RAG flow invloves embedding the user query, retrieving the documents similar to the query, and generate the answer to the query using the retrieved documents. However, in that basic pipeline, RAG does not know whether or not it should retrieve the documents to answer the question, if so, from what data source, do the retrieve documents are actualy relevant to answer the question, or the generated response is correct. As a result, incorrect answers can be generated due to either incorrect retrieval or hallucinations. Therefore, the RAG pipeline should be improved so that it can \"self-reflect\" to decide whether ot not the retrieval should be done and the retrieved information is relevant, and \"self-correct\" to answer the question correctly. To do that, existing research propose [Self-RAG](https://arxiv.org/pdf/2310.11511) and [Coorective RAG (CRAG)](https://arxiv.org/pdf/2401.15884). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Self-RAG](https://arxiv.org/pdf/2310.11511) uses *\"reflection\"* tokens generated by a critic LM after each generation step to indicate whether or not the retrieval is required, whether or not the retrieved documents are relevant to answer the question, whether or not the generated answer is supported by the retrieved documents, and how useful the answer is. \n",
    "\n",
    "| ![self-rag](resources/self_rag.png) | \n",
    "|:--:| \n",
    "| *The difference between the basic RAG pipeline (left) and [Self-RAG](https://arxiv.org/pdf/2310.11511) framework (right)* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, [CRAG](https://arxiv.org/pdf/2401.15884) uses the confidence of the retriever to decide one of three actions, *\"Correct, Ambiguous, Incorrect\"* to decide whether the retrieved contetnt should be used as the context or do a web search to retrieve relevant context. Given a user query and the retrieved documents from any retriever, a lightweight retrieval evaluator (compared to self-RAG) is constructed to estimate the relevance score of retrieved documents to the query. The relevance score is quantified into the aforementioned three actions. If the action \"Correct\" is triggered, the retrieved documents will be refined into more precise knowledge strips. This refinement operation involves knowledge decomposition, filter, and recomposition. If the action \"Incorrect\" is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections. Eventually, when it cannot confidently make a correct or incorrect judgment, a soft and balanced action \"Ambiguous\" which combines both of them is triggered. After optimizing the retrieval results, a generative model is used to generate the answer to the user query.\n",
    "\n",
    "| ![crag](resources/crag.png) | \n",
    "|:--:| \n",
    "| *[CRAG](https://arxiv.org/pdf/2401.15884) workflow* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, the RAG pipeline is now improved to take decisions and act accordingly throughout the process. These kinds of RAGs usually require some kind of feedback, re-generating the question and/or re-retrieving documents. State machines are a kind of cognitive architecture that supports loops and it well suited for this: a state machine simply lets us define a set of steps (e.g., retrieval, grade documents, re-write query) and set the transitions options between them. Therefore, next we implement this new RAG pipeline using [LangGraphs](https://python.langchain.com/v0.1/docs/langgraph/), a recently introduced feature by Langchain according to the following diagram.\n",
    "\n",
    "| ![langgraph](resources/langgraph.png) | \n",
    "|:--:| \n",
    "| *New RAG Workflow: A simplified verision of CRAG (For more information, read the langchain [blog post](https://blog.langchain.dev/agentic-rag-with-langgraph/).)* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize our retriever (Functionality of the node \"Retrieve\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=30\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly we define the functionality for the \"Grade\" node. To get the grading result, we first define a Pydantic model `Grader`. It has one variable `grade` dedicated to indicating whether or not the retrieved document for the question is relevant. Using the model, we create the `grading_chain` that returns a `Grader` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grader(grade='yes')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "question = \"What is agent memory?\"\n",
    "\n",
    "class Grader(BaseModel):\n",
    "    \n",
    "    \"\"\"Returns a binary value 'yes' or 'no' based on the relevancy of the document to the question\"\"\"\n",
    "    \n",
    "    grade: Literal[\"yes\", \"no\"] = Field(..., description=\"The relevancy of the document to the question. 'yes' if relevant, 'no' if not relevant\")\n",
    "    \n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "grading_llm = llm.with_structured_output(Grader)\n",
    "\n",
    "grading_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"),\n",
    "        ('user', \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "grading_chain = grading_prompt | grading_llm\n",
    "\n",
    "\n",
    "grading_chain.invoke({'document': doc_txt, 'question': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask a question that cannot be answered using the documents embedded in the vector store, the chain returns a `Grader` object with the `grade` no. If we want get the grade we can call the  `Grader.grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grader(grade='no')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_chain.invoke({'document': doc_txt, 'question':'What is access control?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we define the functionality of the \"Generate\" node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent memory, in the context of generative agents, refers to a long-term memory module or external database that records a comprehensive list of agents’ experiences in natural language. This memory, combined with planning and reflection mechanisms, enables agents to behave based on past experiences and interact with other agents.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "generation_chain.invoke({'context': doc_txt, 'question': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourthly, we define the functionality of the \"Re-write query\" as a chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"What is agent memory in computer science?\"'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "re_write_chain = re_write_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "re_write_chain.invoke({\"question\": \"agent memory\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initialize the [Tavily](https://tavily.com/) web search tool as the \"Web search\" node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the chains required for the functionalities of each node, we then define the `GraphState` to be used by LangGraph. In LangGraph, a graph is parameterized by a state object that it passes around to each node. Remember that each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with. The default operation is SET, which overrides the existing values of the state.\n",
    "\n",
    "Here our state is a `TypedDict` with 4 keys indicating the question, documents retrieved for the question, whether of not search the web, and the final LLM generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} See also\n",
    ":class: tip\n",
    "For more information about the graph states refer the [documentation](https://python.langchain.com/v0.1/docs/langgraph/#define-the-agent-state). \n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_web: whether to search the web\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question: str\n",
    "    documents: list[str]\n",
    "    search_web: str\n",
    "    generation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through each node, we update the state accordingly. We represent \"nodes\" in the graph as methods, that we create below. For instance, the \"Retrieve\" node is represented as the `retrieve` method, that retrieves the documents according to the graph state's `question` field and updates the `documents` field of the state. Similarly we create a method for each node that use the defined chains to perform a specific task, update the state, and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents related to the question\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    print('> 📃 Retrieving documents...')\n",
    "    \n",
    "    question = state['question']\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    state['documents'] = [doc.page_content for doc in docs]\n",
    "    return state\n",
    "\n",
    "def grade(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    search_web = \"yes\"\n",
    "    \n",
    "    print('> 🔍 Grading documents...')\n",
    "    \n",
    "    filtered_docs = []\n",
    "    \n",
    "    for i,doc in enumerate(documents):\n",
    "        grade = grading_chain.invoke({'document': doc, 'question': question})\n",
    "        if grade.grade == 'yes':\n",
    "            print(f'> 📝 \\033[92mDocument {i} is relevant\\033[0m')\n",
    "            filtered_docs.append(doc)\n",
    "            search_web = 'no'\n",
    "        else:\n",
    "            print(f'> 📝 \\033[91mDocument {i} is irrelevant\\033[0m')\n",
    "\n",
    "            \n",
    "    state['documents'] = filtered_docs\n",
    "    state['search_web'] = search_web\n",
    "    \n",
    "    return state\n",
    "\n",
    "def rewrite_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "    print('> ✍🏻 Rewriting the question...')\n",
    "    question = state['question']\n",
    "    new_question = re_write_chain.invoke({'question': question})\n",
    "    \n",
    "    state['question'] = new_question\n",
    "    return state\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print('> 🌎 Web searching...')\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    \n",
    "    state[\"documents\"] = documents\n",
    "\n",
    "    return state\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"> 🤖 Generating the answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = generation_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that the destination depends on the contents of the graph's state. For instance, after grading the retrieved documents, our RAG pipeline should decide whether to use the retrieved documents to generate the answer or to re-write the query for a web search. It depends on the `search_web` property of the graph's state which is updated within the `grade` node. Therefore, we define our one and only conditional edge `decide_to_generate` which outputs name of the next node to go based on the grading decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns: Method name to execute next\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"> ❓ Deciding to generate...\")\n",
    "    search_web = state[\"search_web\"]\n",
    "\n",
    "    if search_web == \"yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"> 💡 Decision: \\033[91mAll the retrieved documents are irrelevant\\033[0m\")\n",
    "        return \"rewrite_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"> 💡 Decision: \\033[92mRelevant documents found\\033[0m\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining all the nodes and edges, we finally define our graph `workflow` that passes the `GraphState` node to node via edges. We start by adding each node to the graph. Then we define the  `entry_point` (i.e., the root node) that has only one outward edge. From the entry point we connect each node using `add_edge` with the source node name and target node name. In conditional edges, we use `add_conditional_edges` to specify the source node, the decision method, and what node to point based on the output of the decision method as a dictionary. In our case, if the `decide_to_generate` outputs `rewrite_query`, an edge will connect the `grade` and `rewrite_query` nodes. On the other hand, if the `decide_to_generate` outputs `generate`, an edge will connect the `grade` and `generate` nodes. We complete defining our graph with the `generate` node which does not have any outward nodes. So the last edge's target node is a special node named, `END`. \n",
    "\n",
    "After creating the graph we compile it using the `compile()` method. Compiled graph will take the input and go through all the required nodes in order to reach the `END`. We stream the graph's output using the `stream()` method so that we can print to see what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Provide the state graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade\", grade)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"rewrite_query\", rewrite_query)  # rewrite_query\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"rewrite_query\": \"rewrite_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"rewrite_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "def run_pipeline(question):\n",
    "    inputs = {\"question\": question}\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            if key == 'generate':\n",
    "                print()\n",
    "                print(f'Question: {inputs[\"question\"]}')\n",
    "                print(f\"Answer: {value['generation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 📃 Retrieving documents...\n",
      "> 🔍 Grading documents...\n",
      "> 📝 \u001b[92mDocument 0 is relevant\u001b[0m\n",
      "> 📝 \u001b[92mDocument 1 is relevant\u001b[0m\n",
      "> 📝 \u001b[92mDocument 2 is relevant\u001b[0m\n",
      "> 📝 \u001b[92mDocument 3 is relevant\u001b[0m\n",
      "> ❓ Deciding to generate...\n",
      "> 💡 Decision: \u001b[92mRelevant documents found\u001b[0m\n",
      "> 🤖 Generating the answer...\n",
      "\n",
      "Question: What is agent memory?\n",
      "Answer: Agent memory refers to the capability of an agent to retain and recall information over time. It can be short-term, used for in-context learning, or long-term, often leveraging an external database to store and retrieve information over extended periods. This memory allows agents to behave based on past experiences and interact with other agents.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"What is agent memory?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tha LangSmith trace for the above workflow will look like [this](https://smith.langchain.com/public/55f800ce-b505-4b69-85ec-32cfc9b30b14/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 📃 Retrieving documents...\n",
      "> 🔍 Grading documents...\n",
      "> 📝 \u001b[91mDocument 0 is irrelevant\u001b[0m\n",
      "> 📝 \u001b[91mDocument 1 is irrelevant\u001b[0m\n",
      "> 📝 \u001b[91mDocument 2 is irrelevant\u001b[0m\n",
      "> 📝 \u001b[91mDocument 3 is irrelevant\u001b[0m\n",
      "> ❓ Deciding to generate...\n",
      "> 💡 Decision: \u001b[91mAll the retrieved documents are irrelevant\u001b[0m\n",
      "> ✍🏻 Rewriting the question...\n",
      "> 🌎 Web searching...\n",
      "> 🤖 Generating the answer...\n",
      "\n",
      "Question: Who is Tom Brady?\n",
      "Answer: Tom Brady is an NFL football player born on August 3, 1977. He has led the Patriots to multiple victories, including setting an NFL record with 21 straight wins and becoming the first player ever to win six Super Bowls. Brady has also been named to the Pro Bowl nine times in his career and was the oldest quarterback at 41 years of age to win a Super Bowl.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"Who is Tom Brady?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace of the above workflow will look like [this](https://smith.langchain.com/public/995876f0-427b-4a15-9caf-abfd89228630/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
