{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation II\n",
    "\n",
    "In this section we improve the previous CRAG implementation by adding \"Query Analysis\" as described in [Self-RAG](https://arxiv.org/pdf/2310.11511) and [Adaptive RAG](https://arxiv.org/pdf/2403.14403) papers, according to the following graph.\n",
    "\n",
    "| ![arag](resources/arag.png) | \n",
    "|:--:| \n",
    "| *Improved RAG pipeline with Query analysis and Self-RAG* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating our retriever to retrieve documents (documents about agents, prompt engineering, and adverserial attacks on llms) from the vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=30\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the chain that decides whether to redirect the user question to the vectorstore, or to do a web search or to fallback when the user asks a generic question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class QueryRouter(BaseModel):\n",
    "    \"\"\"Routes the user query to appropriate datasources. If the query can be answered using documents about either LLM agents, prompt engineering, or adverserial attacks on LLMs, returns 'vectorstore'. Otherwise returns 'web_serach'. If the query can be answered using LLM's internal knowledge, return 'fallback'\"\"\"\n",
    "    \n",
    "    datasource: Literal[\"vectorstore\", \"web_search\", \"fallback\"] = Field(...,\n",
    "                description=\"The datasource to use for answering the query. 'vectorstore' if the query is either related to LLM agents, prompt engineering, or adverserial attacks on LLMs \\\n",
    "                        'web_search' if the query is not related to the above topics and requires web search. 'fallback' if the query can be answered using LLM's internal knowledge\")\n",
    "                        \n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "query_llm = llm.with_structured_output(QueryRouter)\n",
    "\n",
    "query_router_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an expert at routing a user question to a vectorstore or web search. The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web_search. If the question can be answered using LLM's internal knowledge, use fallback.\\n\\n\n",
    "Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "query_routing_chain = (query_router_prompt | query_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryRouter(datasource='vectorstore')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the types of agent memory?\"\n",
    "\n",
    "query_routing_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryRouter(datasource='web_search')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is tom brady?\"\n",
    "\n",
    "query_routing_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryRouter(datasource='fallback')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hi, how are you?\"\n",
    "\n",
    "query_routing_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we create our `DocumentGrader` that decides whether or not the retrieved documents are relevant to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentGrader(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    grade: str = Field(..., \n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "grader_llm = llm.with_structured_output(DocumentGrader)\n",
    "\n",
    "grading_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n\\n\n",
    "    Retrieved document: {document}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "grading_chain = (grading_prompt | grader_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentGrader(grade='yes')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is agent memory?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "grading_chain.invoke({\"document\": doc_txt, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the chain that would answer the user question based on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prmpt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "qa_chain = qa_prmpt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Agent memory refers to the component of an autonomous agent system that enables the agent to retain and recall information over time. This can include both short-term and long-term memory. The memory stream, a type of long-term memory module, records a comprehensive list of the agent's experiences in natural language.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke({\"question\": \"What is agent memory?\", \"context\": docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the fallback chain that answers the user query using the LLM's internal knowledge without any aditional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. Answer the question based upon your knowledge. Use three sentences maximum and keep the answer concise.\\n\\n\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "fallback_chain = fallback_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an artificial intelligence and don't have feelings, but I'm here and ready to assist you. How can I help you today?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallback_chain.invoke({'question': \"Hi how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the chain the detects whether or not the answer is supported by the retrieved context (i.e., no hallucinations). The `HallucinationEvaluator` returns 'yes' if there are no hallucinations, and 'no' otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallucinationEvaluator(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    grade: str = Field(...,\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "hallucination_llm = llm.with_structured_output(HallucinationEvaluator)\n",
    "hallucination_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\\n\\n\n",
    "    Set of facts: {documents} \\n\\n LLM generation: {generation}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "hallucination_chain = hallucination_prompt | hallucination_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the types agent memory?\n",
      "Generation: The types of agent memory include a memory stream, which is a long-term memory module that records a comprehensive list of agents’ experiences in natural language. There is also a retrieval model that surfaces the context to inform the agent’s behavior, and a reflection mechanism that synthesizes memories into higher level inferences over time. Additionally, agents have short-term memory for in-context learning and long-term memory for retaining and recalling information over extended periods.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HallucinationEvaluator(grade='yes')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the types agent memory?\"\n",
    "print(\"Question: \" + question)\n",
    "generation = qa_chain.invoke({\"question\": question, \"context\": docs})\n",
    "print(\"Generation: \" + generation)\n",
    "hallucination_chain.invoke({\"documents\": docs, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the chain that assess whether or not the answer, correctly answers the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGrader(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    grade: str = Field(...,\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "answer_grader_llm = llm.with_structured_output(AnswerGrader)\n",
    "answer_grader_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a grader assessing whether an answer addresses / resolves a question. \\n\n",
    "    Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\\n\\n\n",
    "    Question: {question} \\n\\n Answer: {answer}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "answer_grader_chain = answer_grader_prompt | answer_grader_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerGrader(grade='yes')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_grader_chain.invoke({\"question\": question, \"answer\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerGrader(grade='no')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_grader_chain.invoke({\"question\": question, \"answer\": \"Tom Brady is an NFL football player born on August 3, 1977. He has led the Patriots to multiple victories, including setting an NFL record with 21 straight wins and becoming the first player ever to win six Super Bowls.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the web search tool for sercing web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the graph state that would be changed while traversing the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the methods for each node of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"> 📃 Retrieving documents...\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"> 🌎 Web searching...\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    \n",
    "    state[\"documents\"] = web_results\n",
    "\n",
    "    return state\n",
    "\n",
    "def fallback(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/o vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"> 👈 Initiating fallback...\")\n",
    "    question = state[\"question\"]\n",
    "    generation = fallback_chain.invoke({\"question\": question})\n",
    "    \n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/ vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"> 🤖 Generating answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = qa_chain.invoke({\"question\": question, \"context\": documents})\n",
    "    \n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"> 🔍 Grading documents...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for i,doc in enumerate(documents):\n",
    "        grade = grading_chain.invoke({'document': doc, 'question': question})\n",
    "        if grade.grade == 'yes':\n",
    "            print(f'> 📝 \\033[92mDocument {i} is relevant\\033[0m')\n",
    "            filtered_docs.append(doc)\n",
    "\n",
    "        else:\n",
    "            print(f'> 📝 \\033[91mDocument {i} is irrelevant\\033[0m')\n",
    "            \n",
    "    state[\"documents\"] = filtered_docs\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the methods for conditional edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    route = query_routing_chain.invoke({\"question\": question})\n",
    "    \n",
    "    if route.datasource == \"vectorstore\":\n",
    "        print(\"> 📚 Routing to the vectorstore...\")\n",
    "        return \"retrieve\"\n",
    "    \n",
    "    elif route.datasource == \"web_search\":\n",
    "        print(\"> 🌎 Routing to web search...\")\n",
    "        return \"web_search\"\n",
    "    \n",
    "    else:\n",
    "        print(\"> 👈 Routing to fallback...\")\n",
    "        return \"fallback\"\n",
    "    \n",
    "    \n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"> 🤔 Deciding to generate...\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        print(\"> 💡 Decision: \\033[91mAll the retrieved documents are irrelevant\\033[0m\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"> 💡 Decision: \\033[92mRelevant documents found\\033[0m\")\n",
    "        return \"generate\"\n",
    "    \n",
    "    \n",
    "def evaluate_response(state):\n",
    "    \n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    print(\"> 🧠 Evaluating the response for hallucinations...\")\n",
    "    \n",
    "    hallucination_grade = hallucination_chain.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    \n",
    "    if hallucination_grade.grade == \"yes\":\n",
    "        print(\"> ✅ \\033[92mGeneration is grounded in the documents\\033[0m\")\n",
    "        \n",
    "        print(\"> 🧠 Evaluating the response for answer...\")\n",
    "        \n",
    "        answer_grade = answer_grader_chain.invoke({\"question\": question, \"answer\": generation})\n",
    "        \n",
    "        if answer_grade.grade == \"yes\":\n",
    "            print(\"> ✅ \\033[92mAnswer addresses the question\\033[0m\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"> ❌ \\033[91mAnswer does not address the question\\033[0m\")\n",
    "            return \"notuseful\"\n",
    "        \n",
    "    else:\n",
    "        print(\"> ❌ \\033[91mGeneration is not grounded in the documents\\033[0m\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the tree and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"fallback\", fallback)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        'retrieve': 'retrieve',\n",
    "        'web_search': 'web_search',\n",
    "        'fallback': 'fallback'\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    'grade_documents',\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        'web_search': 'web_search',\n",
    "        'generate': 'generate'\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    'generate',\n",
    "    evaluate_response,\n",
    "    {\n",
    "        'useful': END,\n",
    "        'notuseful': 'web_search',\n",
    "        'not supported': 'generate'\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "def run_pipeline(question):\n",
    "    inputs = {\"question\": question}\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            if key == 'generate' or key == 'fallback':\n",
    "                print()\n",
    "                print(f'Question: {inputs[\"question\"]}')\n",
    "                print(f\"Answer: {value['generation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 📚 Routing to the vectorstore...\n",
      "> 📃 Retrieving documents...\n",
      "> 🔍 Grading documents...\n",
      "> 📝 \u001b[92mDocument 0 is relevant\u001b[0m\n",
      "> 📝 \u001b[91mDocument 1 is irrelevant\u001b[0m\n",
      "> 📝 \u001b[92mDocument 2 is relevant\u001b[0m\n",
      "> 📝 \u001b[92mDocument 3 is relevant\u001b[0m\n",
      "> 🤔 Deciding to generate...\n",
      "> 💡 Decision: \u001b[92mRelevant documents found\u001b[0m\n",
      "> 🤖 Generating answer...\n",
      "> 🧠 Evaluating the response for hallucinations...\n",
      "> ✅ \u001b[92mGeneration is grounded in the documents\u001b[0m\n",
      "> 🧠 Evaluating the response for answer...\n",
      "> ✅ \u001b[92mAnswer addresses the question\u001b[0m\n",
      "\n",
      "Question: What are the types of agent memory?\n",
      "Answer: The types of agent memory include sensory memory, short-term memory, and long-term memory. Sensory memory retains impressions of sensory information such as visual, auditory, and touch stimuli for a few seconds. Short-term memory is used for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods, often by leveraging an external vector store and fast retrieval.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"What are the types of agent memory?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the above workflow will look like [this](https://smith.langchain.com/public/14f935da-bafb-42e9-b863-f545cc6f8485/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 📚 Routing to the vectorstore...\n",
      "> 📃 Retrieving documents...\n",
      "> 🔍 Grading documents...\n",
      "> 📝 \u001b[91mDocument 0 is irrelevant\u001b[0m\n",
      "> 📝 \u001b[91mDocument 1 is irrelevant\u001b[0m\n",
      "> 📝 \u001b[91mDocument 2 is irrelevant\u001b[0m\n",
      "> 📝 \u001b[91mDocument 3 is irrelevant\u001b[0m\n",
      "> 🤔 Deciding to generate...\n",
      "> 💡 Decision: \u001b[91mAll the retrieved documents are irrelevant\u001b[0m\n",
      "> 🌎 Web searching...\n",
      "> 🤖 Generating answer...\n",
      "> 🧠 Evaluating the response for hallucinations...\n",
      "> ✅ \u001b[92mGeneration is grounded in the documents\u001b[0m\n",
      "> 🧠 Evaluating the response for answer...\n",
      "> ✅ \u001b[92mAnswer addresses the question\u001b[0m\n",
      "\n",
      "Question: How to continually pre-train an LLM?\n",
      "Answer: To continually pre-train a Large Language Model (LLM), you can use a method referred to as \"re-warming\" the model. This involves re-increasing a small learning rate to continue training the pre-trained model on new data. Additionally, you can add a small portion of the original pre-training data to the new dataset to prevent catastrophic forgetting.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"How to continually pre-train an LLM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the above workflow will look like [this](https://smith.langchain.com/public/a5a9d1a7-4190-433e-9eb6-f1339870abb4/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 🌎 Routing to web search...\n",
      "> 🌎 Web searching...\n",
      "> 🤖 Generating answer...\n",
      "> 🧠 Evaluating the response for hallucinations...\n",
      "> ✅ \u001b[92mGeneration is grounded in the documents\u001b[0m\n",
      "> 🧠 Evaluating the response for answer...\n",
      "> ✅ \u001b[92mAnswer addresses the question\u001b[0m\n",
      "\n",
      "Question: Who is Bobby Lee?\n",
      "Answer: Bobby Lee, born on September 17, 1972, is an American stand-up comedian, actor, and podcaster. He is known for his roles in \"A Very Harold & Kumar Christmas\", \"Mad TV\", and \"Paul\". He also co-hosts the podcasts Tigerbelly and Bad Friends.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"Who is Bobby Lee?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the above workflow will look like [this](https://smith.langchain.com/public/7ec11744-7e30-469d-842e-dea6c89d9a59/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 👈 Routing to fallback...\n",
      "> 👈 Initiating fallback...\n",
      "\n",
      "Question: Hi, how are you?\n",
      "Answer: I'm an artificial intelligence and don't have feelings, but I'm here and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(\"Hi, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the above workflow will look like [this](https://smith.langchain.com/public/d8b8fc93-fee7-46ee-8350-373f3bd45f38/r)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
