{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "When we have multiple data sources such as a GraphDB, PDF documents (i.e., a vector store), we might need to answer user queries based on the correct data source. For example, if the user wants to know about reviews of a hospital, user query should be redirected to the vector store containing embeddings of hospital reviews. On the other hand, if the user wants to know about information such as the doctors, patients, their visits to the hospital, the user query should probably be send to a graph database that contains the hospial information. Therefore, to provide such as functionality we will now focus on \"Routing\" in RAG with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we talk about two main types of routing techniques, namely **Logical routing** and **Semantic routing**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import our libraries and create two vector stores to where we re-direct the user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakunaharinda/Documents/Repositories/ragatouille/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def generate_vectorstores(file, dir):\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split text into chunks\n",
    "\n",
    "    text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                        embedding=OpenAIEmbeddings(),\n",
    "                                        persist_directory=dir)\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# Create a vectorstore to answer questions about LoRA\n",
    "vectorstore_lora = generate_vectorstores(\"data/LoRA.pdf\",\"data/vectorstore_lora\")\n",
    "\n",
    "# Create a vectorstore to answer questions about BERT\n",
    "vectorstore_bert = generate_vectorstores(\"data/BERT.pdf\",\"data/vectorstore_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_lora = vectorstore_lora.as_retriever(search_kwargs={'k':5})\n",
    "retriever_bert = vectorstore_bert.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical Routing\n",
    "\n",
    "In logical routing we let the LLM to decide the route based on a set of pre-defined options/routes. To do that we first define our router with three main routes as a Pydantic [model](https://docs.pydantic.dev/latest/concepts/models/). In the `QueryRouter` model, we define 2 fields, namely `datasource` indicating the datasource where the query is re-directed to and the `question` representing the user query. For the `datasource` field, we allow three values \"lora\", \"bert\" that represent two vectore stores we created earlier, and \"general\" to route the query directly to the LLM as the fallback mechanism.\n",
    "\n",
    "After specifying our router we initialize our LLM as GPT-4 to provide the output as a `QueryRouter` object using `with_structured_output()` method. \n",
    "\n",
    "Finally, we crate our router chain using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "\n",
    "class QueryRouter(BaseModel):\n",
    "    \n",
    "    \"\"\"Route a user query to the appropriate datasource that will help answer the query accurately\"\"\"\n",
    "    \n",
    "    datasource: Literal['lora', 'bert', 'general'] = Field(..., \n",
    "                                                description=\"Given a user question choose which datasource would be most relevant for answering their question\"\n",
    "                                                )\n",
    "    question: str = Field(..., description=\"User question to be routed to the appropriate datasource\")\n",
    "    \n",
    "llm = ChatOpenAI(model='gpt-4',temperature=0)\n",
    "structured_llm = llm.with_structured_output(QueryRouter)\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert router that can direct user queries to the appropriate datasource. Route the following user question about a topic in NLP and LLMs to the appropriate datasource.\\nIf it is a general question not related to the provided datasources, route it to the general datasource.\\n\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | router_prompt\n",
    "    | structured_llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After invoking our router chain we will be able to see it logically decides the datasource to redirect the query and output it as a `QueryRouter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryRouter(datasource='bert', question='How does the BERT work?')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How does the BERT work?\"\n",
    "result = router.invoke(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to use the router output and perform the QA accordingly, we define a new method `choose_route`. `choose_route` checks the router chain result to extract the datasource and defines three chains to answer the questions related to BERT, LoRA, and general domain. \n",
    "\n",
    "We complete our RAG with one final chain by putting all the methods and chains together in the `full_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "\n",
    "def choose_route(result):\n",
    "    \n",
    "    llm_route = ChatOpenAI(model='gpt-4',temperature=0)\n",
    "    if \"bert\" in result.datasource.lower():\n",
    "        print(f\"> Asking about BERT ...\\nQuestion: {result.question}\\nAnswer:\")\n",
    "        bert_chain = (\n",
    "            {'context': retriever_bert, 'question': RunnablePassthrough()}\n",
    "            | qa_prompt\n",
    "            | llm_route\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return bert_chain.invoke(result.question)\n",
    "    elif \"lora\" in result.datasource.lower():\n",
    "        print(f\"> Asking about LoRA ...\\nQuestion: {result.question}\\nAnswer:\")\n",
    "        lora_chain = (\n",
    "            {'context': retriever_lora, 'question': RunnablePassthrough()}\n",
    "            | qa_prompt\n",
    "            | llm_route\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return lora_chain.invoke(result.question)\n",
    "    else:\n",
    "        print(f\"> Asking about a general question ...\\nQuestion: {result.question}\\nAnswer:\")\n",
    "        general_chain = llm_route | StrOutputParser()\n",
    "        return general_chain.invoke(result.question)\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Asking about LoRA ...\n",
      "Question: What are the benefits of LoRA?\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"LoRA, or Low-Rank Adaptation, offers several benefits. It makes training more efficient and lowers the hardware barrier to entry by up to three times, as it doesn't require the calculation of gradients or maintenance of optimizer states for most parameters. It also allows for quick task-switching when deployed as a service by sharing the majority of the model parameters, and it reduces the number of trainable parameters and the GPU memory requirement, without introducing additional inference latency.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke(\"What are the benefits of LoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for our logical router will look like [this](https://smith.langchain.com/public/8a9330a0-5254-4602-8c26-5c9baa158eb5/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
