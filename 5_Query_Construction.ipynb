{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Construction\n",
    "\n",
    "After the user asks a question in natural language and routed to a specific datasource (e.g., vector store, Graph database, etc,), the question should be transformed into a structured query to retrieve information from the selected datasource (e.g., Text-to-SQL, Text-to-Cypher, etc.). In this section we will see how the query construction can be done using Langchain to transform the natural language question into a structured query. \n",
    "\n",
    "A detailed blog post published by Langchain can be found [here](https://blog.langchain.dev/query-construction/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we wil see how the natural language question can be used to construct a query to filter records using their meta data. To do that we consider a use case where we need to filter a YouTube playlist based on meta data for its videos. Let's first find out what are meta data we can find in a YouTube video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'sVcwVQRHIc8',\n",
       " 'title': 'Learn RAG From Scratch â€“ Python AI Tutorial from a LangChain Engineer',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 118979,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/sVcwVQRHIc8/hq720.jpg',\n",
       " 'publish_date': '2024-04-17 00:00:00',\n",
       " 'length': 9191,\n",
       " 'author': 'freeCodeCamp.org'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=sVcwVQRHIc8\", add_video_info=True\n",
    ").load()\n",
    "\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assume that we are going to filter the playlist based on the `title`, `content`, `view_count`, `publish_date`, and `length`. So, once we ask a question from the LLM, it should return an object containing above fields which is used to select videos from a playlist/database. To do that similar to the one we built in the \"Routing\" section, we define a Pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database/playlist of videos about LLMs and Langchain.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build a chain that takes the user question with a prompt, provide it to an LLM that provides answers according to the defined Pydantic model, and return the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: Build RAG using Langchain\n",
      "title_search: Build RAG Langchain\n",
      "min_view_count: 1000\n",
      "latest_publish_date: 2024-01-01\n"
     ]
    }
   ],
   "source": [
    "meta_data_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert at converting user questions into database queries. \\\n",
    "                    You have access to a database of tutorial videos about LLMs and Langchain. \\\n",
    "                    Given a question, return a database query optimized to retrieve the most relevant results.\"\n",
    "        ),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4',temperature=0)\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "\n",
    "meta_data_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | meta_data_prompt\n",
    "    | structured_llm\n",
    ")\n",
    "\n",
    "query = meta_data_chain.invoke(\"Build RAG using Langchain videos published before January 2024 with at least 1000 views.\")\n",
    "query.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to filter the records of the database as shown in this [article](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/#constructing-from-scratch-with-lcel). We will be using a Text-to-Cypher query construction in our final section to get information from a Graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
