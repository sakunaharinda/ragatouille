{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Translation\n",
    "\n",
    "The main idea behind the Query Translation is that translate the user query in a way that the LLM can correctly answer the question. For instance, if the user asks an ambiguous question, our RAG rretriever might retrieve incorrect (or ambiguous) documents based on the embeddings that are not very relevant to answer the user question, leading the LLM to hallucinate answers. There are few ways to tackle this problem. Some of them are,\n",
    "\n",
    "- [Step-back prompting](https://arxiv.org/pdf/2310.06117): This involves encouraging the LLM to take a step back from a given question or problem and pose a more abstract, higher-level question that encompasses the essence of the original inquiry.\n",
    "- [Least-to-most prompting](https://arxiv.org/pdf/2205.10625): This allows to break down a complex problem into a series of simpler subproblems and then solve them in sequence Solving each subproblem is facilitated by the answers to previously solved subproblems.\n",
    "- Query re-writing ([Multi-Query](https://medium.com/@kbdhunga/advanced-rag-multi-query-retriever-approach-ad8cd0ea0f5b) or [RAG Fusion](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)): This allows to generate multiple questions from the original question with different wording and perspectives. Then retrieve documents using the similarity scores between each question and the vector store to answer the orginal question.\n",
    "\n",
    "Now, let's try to implement the above techniques using LangChain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv secrets/secrets.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Introduction notebook, we first import the libraries, load documents, split them, generate embeddings, store them in a vector store and create the retriever using the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakunaharinda/Documents/Repositories/ragatouille/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('data/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query\n",
    "\n",
    "In multi-query approach, we first use an LLM (here it is an instance of GPT-4) to generate 5 different questions based on our original question. To do that, we create a prompt and encapsulate it with the `ChatPromptTemplate`. Then we create the chain using LCEL, to read the user input and assign it to the `question` placeholder of the prompt, send the prompt to the LLM, parse the output containing 5 questions seperated by new line charcters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an intelligent assistant. Your task is to generate 5 questions based on the provided question in different wording and different perspectives to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether or not our query generation works by invoking the created chain with a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you provide information on QLoRA?',\n",
       " 'What does QLoRA stand for?',\n",
       " 'Can you explain the concept of QLoRA?',\n",
       " 'Could you elaborate on what QLoRA is?',\n",
       " 'What are the details about QLoRA?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries.invoke(\"What is QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get the 5 questions, we parallely retrieve the most relevant 5 documents for each question (resulting in a list of lists) and create a new document list by taking the unique documents of the union of all the retrieved documents. To do that we create another chain, `retrieval_chain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import loads, dumps\n",
    "\n",
    "def get_context_union(docs: List[List]):\n",
    "    all_docs = [dumps(d) for doc in docs for d in doc]\n",
    "    unique_docs = list(set(all_docs))\n",
    "    \n",
    "    return [loads(doc).page_content for doc in unique_docs] # We only return page contents\n",
    "\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | generate_queries\n",
    "    | retriever.map()\n",
    "    | get_context_union\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models',\n",
       " 'investigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7 Related Work\\nQuantization of Large Language Models Quantization of LLMs has largely focused on quanti-\\nzation for inference time. Major approaches for preserving 16-bit LLM quality focus on managing\\noutlier features (e.g., SmoothQuant [ 66] and LLM.int8() [ 14]) while others use more sophisticated',\n",
       " 'Quantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA',\n",
       " 'There are many directions for future works. 1) LoRA can be combined with other efﬁcient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\\nor LoRA is far from clear – how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁne-\\n12',\n",
       " 'finetuning models. The main question now is whether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5',\n",
       " 'A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA αfixed and search the learning rate, since\\nLoRA αis always proportional to the learning rate.',\n",
       " 'construction.\\n• LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preﬁx-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the',\n",
       " 'technology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis',\n",
       " 'All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the',\n",
       " 'LoRA, an efﬁcient adaptation strategy that neither introduces inference latency nor reduces input\\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\\non Transformer language models, the proposed principles are generally applicable to any neural\\nnetworks with dense layers.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"What is QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we put all together by creating a one final chain to read the user query, get the contexts from 5 different documents using the `retrieval_chain`, add both the question and context to the prompt, send it through the LLM, and get the final formatted output using  the `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Asnwer the given question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "multi_query_chain = (\n",
    "    {'context': retrieval_chain, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLoRA is an efficient adaptation strategy for language models that allows for quick task-switching when deployed as a service by sharing the majority of the model parameters. It does not introduce inference latency nor reduces input sequence length while retaining high model quality. QLoRA introduces a number of innovations to save memory without sacrificing performance, such as 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights. It enables the finetuning of large language models on phones and other low resource settings, making the finetuning of high quality LLMs much more widely and easily accessible.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_chain.invoke(\"What is QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing all the above cells, you will be able to see a LangSmith trace like [this](https://smith.langchain.com/public/f38c02d1-23a5-4961-a076-3ff20a872d45/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
